---
title: "limma core"
author: "gyzhou"
date: "23/Jun/2016"
output: html_document
---

This file reproduces key algorithm implemented in limma package. First, we simulate the data to use and define the design matrix.
```{r}
library(limma)
set.seed(100)
sd <- 0.3*sqrt(4/rchisq(100,df=4))
y <- matrix(rnorm(100*6,sd=sd),100,6)
rownames(y) <- paste("Gene",1:100)
y[1:2,4:6] <- y[1:2,4:6] + 2
design <- factor(c("A", "A", "B", "B", "C", "C"))
(design <- model.matrix(~0+design))
```

## Statistical Methods for linear models
#### Basic properties of linear models
We use multiple linear regression model: $$Y_i = \alpha_i x_i + \varepsilon_i, i=1,\dots,6$$
We can use linear algebra to represent this model: 
$$\mathbf{Y} = \begin{pmatrix} Y_1\\ Y_2\\ \vdots\\ Y_6 \end{pmatrix}, \mathbf{X} = \begin{pmatrix} 1&0&0 \\ 1&0&0 \\ 0&1&0 \\ 0&1&0 \\ 0&0&1 \\ 0&0&1 \end{pmatrix}, \boldsymbol{\alpha} = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}, \boldsymbol{\varepsilon} = \begin{pmatrix} \varepsilon_1\\ \varepsilon_2\\ \vdots\\ \varepsilon_6 \end{pmatrix}$$
and $$\mathbf{Y} = \mathbf{X} \boldsymbol{\alpha} + \boldsymbol{\varepsilon}$$.

The mathematical theory tells us that the unbiased least quaires estimate of $\alpha$ can be computed with following equation:
$$\hat{\alpha} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}$$
Also, under the assumption that the errors have mean zero and are uncorrelated with constant variance $\sigma^2$, the variance-covariance matrix of the least squares estimate $\hat{\alpha}$ is $$\Sigma_{\hat{\alpha} \hat{\alpha}} = \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1}$$

#### Estimation of $\sigma^2$
The vector of residuals is $$\hat{\varepsilon} = \mathbf{Y} - \mathbf{X} \hat{\alpha}$$
It can be proved that $s^2 = \frac{1}{N-p} \sum_{i=1}^N \hat{\varepsilon}^2$ is the unbiased estimator of $\sigma^2$.

#### Inference about $\alpha$
Because the assumption that $\varepsilon_i$ are independent and normally distributed and $\hat{\alpha}$ is the linear combination of iid normal variables, they are also normally distributed.Thus, each component $\hat{\alpha}_i$ of $\hat{\alpha}$ is normally distributed with mean $\alpha_i$ and variance $\sigma^2 c_{ii}$, where $\mathbf{C} = (\mathbf{X}^T \mathbf{X})^{-1}$. The standard error of $\hat{\alpha_g}_i$ can thus be estimated as $$s_{\hat{\alpha}_i} = s \sqrt{c_{ii}}$$
Under the normality assumption it can be shown that $$\frac{{\hat{\alpha}_i} - {\hat{\alpha}}} {s_{\hat{\alpha}_i}} \sim t_{n-p}$$

#### Practice with inference about $\alpha_g$
We then go through the core algorithm the limma takes. We first want to obtain the coefficient estimators $\hat{\alpha_g}$. The following three methods produce same results as they apply the same principle, although in fact, qr factorization implemented in `lmFit` and `lm` is an optimized method which works much faster for large datasets.
```{r}
fit <- lmFit(y[1,], design)
fit$coef
lm(y[1,] ~ 0 + design)
solve(crossprod(design)) %*% t(design) %*% y[1,]
```

Next we try to get the estimators $s_g^2$ of $\sigma_g^2$ and the estimated variance-covariance matrix. Again from the theorm above we get $$\mbox{Var} (\hat{\alpha_g}) = \mathbf{V}_g s_g^2$$ where $s_g^2 = \frac{1}{N-P} \sum_{i=1}^N \hat{\varepsilon}^2$ and $\mathbf{V}_g = (\mathbf{X}^T \mathbf{X})^{-1}$.
The following three methods produce same results as they apply the same principle.
```{r}
## by definition
sigma2 <- sum((y[1,] - design %*% t(fit$coef))^2) ## sum of residuals
s2 <- sigma2 / (6 - 3) ## unbiased estimator for sigma^2
sqrt(solve(crossprod(design))) * sqrt(s2) ## unbiased estimator for sigma

## lm
summary(lm(y[1,] ~ 0 + design))

## lmFit
fit$stdev.unscaled ## numeric matrix conformal with coef containing the unscaled standard deviations for the coefficient estimators. 
sqrt(solve(crossprod(design)))

fit$sigma ## numeric vector containing the residual standard deviation for each gene. sg
sqrt(s2)

fit$sigma * fit$stdev.unscaled ## estimators of standard deviation of \alpha_g
```

#### Theory of contrast matrix
We first define the contrast matrix. For example, We are interested in comparison of $B-A$ and $C-B$, then our contrast matrix $\mathbf{C}$ is $\begin{pmatrix} -1&0 \\ 1&-1 \\ 0&1 \end{pmatrix}$, where $$\mathbf{C}^T \alpha = \begin{pmatrix} \alpha_2 - \alpha_1 \\ \alpha_3 - \alpha_2 \end{pmatrix} = \boldsymbol{\beta}$$
Then the contrast estimators $\hat{\boldsymbol{\beta_g}} = \mathbf{C}^T \hat{\boldsymbol{\alpha_g}}$ has variance-covariance matrix:
$$\mbox{Var} (\hat{\boldsymbol{\beta_g}}) = \mathbf{C}^T \mathbf{V}_g \mathbf{C} s_g^2$$
As the contrast estimators are linear combination of coefficient estimators, they are also assumed to be normally distributed with mean $\boldsymbol{\beta_g}$ and variance-covariance matrix $\mathbf{C}^T \mathbf{V}_g \mathbf{C} \sigma^2$. The residual variances $s_g^2$ are assumed to follow approxiamately a scale chisquiare distribution.

#### Practice of contrast matrix
```{r}
(con <- makeContrasts(B.VS.A = designB - designA, 
                     C.VS.A = designC - designA,
                     levels = colnames(design)))
## limma
contrasts.fit(fit, con)$coef

## by definition
t(con) %*% t(fit$coef)
fit$coef %*% con

## limma
contrasts.fit(fit, con)$cov ## \mathbf{C}^T \mathbf{V}_g \mathbf{C}

## by definition
t(con) %*% solve(crossprod(design)) %*% con * s2
contrasts.fit(fit, con)$cov * s2
```

#### Distriubtion and inference 
Let $v_{gj}$ be the jth diagnal element of $\mathbf{C}^T \mathbf{V}_g \mathbf{C}$. The  distribution of $\hat{\beta}_g$ can be summarized by:
$$\hat{\beta}_{gj} | \beta_{gj}, \sigma^2 \sim N(\beta_{gj}, v_{gj} \sigma^2)$$ and
$$s_g^2 | \sigma_g^2 \sim \frac{\sigma_g^2}{d_g} \chi_{d_g}^2$$
where $d_g$ is the residual degrees of freedom for linear models of gene g ($N-P$). Under these assumptions, t-statistics can be calculated with $d_{g}$ degrees of freedom:
$$t_{gj} = \frac{\hat{\beta}_{gj}} {s_g \sqrt{v_{gj}}}$$
For null hypothesis, test $H_0: \beta_{gj} = 0$.

```{r}
## by definition
beta_hat <- contrasts.fit(fit, con)$coef
s_g <- fit$sigma       
v_gj <- diag(contrasts.fit(fit, con)$cov)
(t = beta_hat / (s_g * sqrt(v_gj)))
2 * pt(t, 6-3, lower.tail = F)
```

## Hierarchical Models
limma uses emprical bayes to allow robust estimation of standard error when sample size is small. The key is to define the prior distributions for unknown coefficients $\hat{\beta{gj}}$ and unknown variances $\sigma_g^2$.

We can pool the whole genes to derive infomation for prior distriubtions. The prior distribution, which describes randomness in picking a gene is:
$$\frac{1}{\sigma_g^2} \sim \frac{1}{d_0 s_0^2} \chi_{d_0}^2$$
The sampling distribution, which describes the randomness of the variance of a gene in different sample, is defined in the previous section:
$$s_g^2 | \sigma_g^2 \sim \frac{\sigma_g^2}{d_g} \chi_{d_g}^2$$

Limma also assumes that for any given $j$, a $\beta_{gj}$ is non zero with known probability 
$$\mbox{Pr} (\beta_{gj} \ne 0) = p_j$$
where $p_j$ is the expected proportion of truly differentially expressed genes. Then for these portion of genes, prior information on the coefficient is assumed equivalent to a prior observation equal to zero (assume approximately same number of upregulated and downregulated genes) with unscaled variance $v_{0j}$,
$$\beta_{gj} | \sigma_g^2, \beta_{gj} \ne 0 \sim N(0, v_{0j} \sigma_g^2)$$
Under the Hierarchical model, we now want to compute the bayesian estimator $\mbox{E} (\sigma_g^2 | s_g^2)$ of $\sigma_g^2$. We can use bayesian theroy to derive that:
$$\begin{align} f_{\sigma_g^2 | s_g^2} (\sigma_g^2 | s_g^2) = \frac{f_{s_g^2 | \sigma_g^2} (s_g^2 | \sigma_g^2) f_{\sigma_g^2} (\sigma_g^2)} {\int f_{s_g^2 | \sigma_g^2} (s_g^2 | \sigma_g^2) f_{\sigma_g^2} (\sigma_g^2) \, d(\sigma_g^2)} \end{align}$$

It is easy to derive that:
$$\begin{align} f_{s_g^2 | \sigma_g^2} (s_g^2 | \sigma_g^2) &= f_{s_g^2 | \sigma_g^2}(\frac{d_g}{\sigma_g^2} s_g^2) \frac{d_g}{\sigma_g^2} \\ 
&= (\frac{d_g}{2\sigma_g^2})^{d_g/2} \frac{s_g^{2(d_g/2-1)}} {\Gamma(d_g/2)} \mbox{exp} (- \frac{d_g s_g^2} {2\sigma_g^2}) \end{align}$$

$$\begin{align} f_{\sigma_g^2} (\sigma_g^2) &= f_{\sigma_g^2} (\frac{\sigma_g^2} {d_0 s_0^2}) \frac{1}{d_0 s_0^2} \\
&= (\frac{d_0 s_0^2}{2})^{d_0/2} \frac{\sigma_g^{2(-d_0/2-1)}} {\Gamma(d_0/2)} \mbox{exp} (- \frac{d_0 s_0^2} {2\sigma_g^2})
\end{align}$$

The intergrand thus is 
$$(\frac{d_g}{2\sigma_g^2})^{d_g/2} \frac{s_g^{2(d_g/2-1)}} {\Gamma(d_g/2)} \mbox{exp} (- \frac{d_g s_g^2} {2\sigma_g^2}) \\
\times (\frac{d_0 s_0^2}{2})^{d_0/2} \frac{\sigma_g^{-2(d_0/2-1)}} {\Gamma(d_0/2)} \mbox{exp} (- \frac{d_0 s_0^2} {2\sigma_g^2}) \\
= \frac{(d_0 s_0^2 / 2)^{d_0/2} (d_g/2)^{d_g/2} s_g^{2(d_g/2 - 1)}} {\Gamma(d_g/2) \Gamma(d_0/2)} \sigma_g^{2(-d_0/2 - d_g/2 - 1)} \mbox{exp} \{- (d_g s_g^2 + d_0 s_0^2)/{2 \sigma_g^2} \}$$

Using known definite integration $\int_0^\infty x^a e^{-b/x} \, dx = \int_0^\infty x^{-a-2} e^{-bx} \, dx = b^{a+1} \Gamma(-a-1), (a<0, b>0)$, we get the integrand integrates to:
$$\frac{ (d_0 s_0^2/2)^{d_0/2} (d_g/2)^{d_g/2} s_g^{2(d_g/2-1)}  \Gamma(d_g/2 + d_0/2)} {\Gamma(d_g/2) \Gamma(d_0/2)} (\frac{d_g s_g^2 + d_0 s_0^2} {2})^{-(d_g/2 + d_0/2)}$$

Thus, we get:
$$\begin{align} f_{\sigma_g^2 | s_g^2} (\sigma_g^2 | s_g^2) &= \frac{1}{\sigma_g^2 \Gamma(d_g/2+d_0/2)} (\frac{d_g s_g^2 + d_0 s_0^2} {2 \sigma_g^2})^{(d_0+d_g)/2} \mbox{exp} \{ -(d_g s_g^2 + d_0 s_0^2) / {2 \sigma_g^2} \} \end{align}$$

The pdf of $\chi^2$ with $d_f = d_0+d_g$ and $\chi^2 = \frac{d_g s_g^2 + d_0 s_0^2} {\sigma_g^2}$ is:
$$\frac{1} {d_g s_g^2 + d_0 s_0^2} \frac{\sigma_g^2} {\Gamma(d_g/2+d_0/2)} (\frac{d_g s_g^2 + d_0 s_0^2} {2 \sigma_g^2})^{(d_0+d_g)/2} \mbox{exp} \{ -(d_g s_g^2 + d_0 s_0^2) / {2 \sigma_g^2} \}$$

Thus, 
$$\sigma_g^2 | s_g^2 \sim \frac{d_g s_g^2 + d_0 s_0^2} {\chi_{d_g + d_0}^2}$$
And, $$\tilde{s_g}^{-2} = \mbox{E} (\sigma_g^{-2} | s_g^2) = \frac{d_g s_g^2 + d_0 s_0^2} {d_g + d_0}$$

Now define the moderate t-statistic as:

The posterior mean shrinks the observed variance $s_g^2$ towards the global variance $s_0^2$ and the weights depend on the sample size through the degrees of freedom $d_g$ and the shape of the prior distribution through $d_0$. 


#### Marginal distribution
Now we compute the null joint distribution of $\hat{\beta}_{gj}$ and $s_g^2$, here assuming $\hat{\beta}_{gj}$ and $s_g^2$ is independen under the conditional distribution, we get:
$$f(\hat{\beta}_{gj}, s_g^2 | \beta_{gj} = 0) = \int f(\hat{\beta}_{gj} | s_g^2, \beta = 0) f(s_g^2 | \sigma_g^2) f(\sigma_g^2) \, d(\sigma_g^2) $$

The intergrand is:
$$\frac{1} {(2 \pi v_{gj} \sigma_g^2)^{1/2}} \mbox{exp} (- \frac{\hat{\beta}_{gj}^2} {2 v_{gj} \sigma_g^2}) \\ \times (\frac{d_g}{2\sigma_g^2})^{d_g/2} \frac{s_g^{2(d_g/2-1)}} {\Gamma(d_g/2)} \mbox{exp} (- \frac{d_g s_g^2} {2\sigma_g^2}) \\ \times
(\frac{d_0 s_0^2}{2})^{d_0/2} \frac{\sigma_g^{2(-d_0/2-1)}} {\Gamma(d_0/2)} \mbox{exp} (- \frac{d_0 s_0^2} {2\sigma_g^2}) \\
= \frac{(d_0 s_0^2 / 2)^{d_0/2} (d_g/2)^{d_g/2} s_g^{2(d_g/2 - 1)}} {(2 \pi v_{gj})^{1/2} \Gamma(d_g/2) \Gamma(d_0/2)} \sigma_g^{2(-1/2 -d_0/2 - d_g/2 - 1)} \mbox{exp} \{- ( \frac{\hat{\beta}_{gj}^2} {v_{gj}} + d_g s_g^2 + d_0 s_0^2)/{2 \sigma_g^2} \}$$

which integrates to:
$$f(\hat{\beta}_{gj}, s_g^2 | \beta_{gj} = 0) \\= 
(1/{2 \pi v_{gj}})^{1/2} (d_0 s_0^2 / 2)^{d_0/2} (d_g/2)^{d_g/2} s_g^{2(d_g/2 - 1)} \frac{\Gamma(1/2 + d_0/2 + d_g/2)} {\Gamma(d_0/2) \Gamma(d_g/2)} (\frac{\hat{\beta}_{gj}^2 / v_{gj} + d_0 s_0^2 + d_g s_g^2}{2})^{-(1 + d_0 + d_g)/2}$$
Clearly $\hat{\beta}_{gj}$ and $s_g^2$ is not no longer independent under the unconditional distribution.

The null joint contribution of $\tilde{t}_{gj}$ and $s_g^2$ is
$$f(\tilde{t}_{gj}, s_g^2 | \beta_{gj} = 0) = \tilde{t}_{gj} v_{gj} f(\hat{\beta}_{gj}^2, s_g^2 | \beta = 0)$$ 
which after collection of factors yields
$$f(\tilde{t}_{gj}, s_g^2 | \beta_{gj} = 0) = \frac{(d_0 s_0^2)^{d_0/2} d_g^{d_g/2} s_g^{2(d_g/2-1)}} {B(d_g/2, d_0/2) (d_0 s_0^2 + d_g s_g^2)^(d_0/2 + d_g/2)} \\ \times
\frac{(d_0 + d_g)^{-1/2}} {B(1/2, d_0/2+d_g/2)} (1 + \frac{\tilde{t}_{gj}^2} {d_0+d_g})^{-(1+d_0+d_g)/2}
$$

This shows that $\tilde{t}_{gj}$ and $s_g^2$ are independent with:
$$s_g^2 \sim s_0^2 F_{d_g,d_0}$$
and $$\tilde{t}_{gj} | \beta_{gj} = 0 \sim t_{d_g+d_0}$$

The marginal distribution of $\tilde{t}_{gj}$ over all the genes is therefore a mixture of a scaled t-distibution and an ordinary t-distribution with mixing proportions $p$ and $1-p$.

#### Posterior Odds






